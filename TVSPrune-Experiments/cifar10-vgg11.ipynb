{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e719959c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:03.401700Z",
     "iopub.status.busy": "2023-07-09T17:56:03.400986Z",
     "iopub.status.idle": "2023-07-09T17:56:03.405783Z",
     "shell.execute_reply": "2023-07-09T17:56:03.404939Z"
    },
    "papermill": {
     "duration": 0.016579,
     "end_time": "2023-07-09T17:56:03.410066",
     "exception": false,
     "start_time": "2023-07-09T17:56:03.393487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a9096b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:03.421446Z",
     "iopub.status.busy": "2023-07-09T17:56:03.421047Z",
     "iopub.status.idle": "2023-07-09T17:56:09.179088Z",
     "shell.execute_reply": "2023-07-09T17:56:09.178113Z"
    },
    "papermill": {
     "duration": 5.766365,
     "end_time": "2023-07-09T17:56:09.181719",
     "exception": false,
     "start_time": "2023-07-09T17:56:03.415354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.linalg import svdvals\n",
    "\n",
    "import numpy as np\n",
    "#import powerlaw\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils import prune as prune\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#import kneed as kneed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c935ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:09.194615Z",
     "iopub.status.busy": "2023-07-09T17:56:09.194025Z",
     "iopub.status.idle": "2023-07-09T17:56:09.206452Z",
     "shell.execute_reply": "2023-07-09T17:56:09.205457Z"
    },
    "papermill": {
     "duration": 0.021185,
     "end_time": "2023-07-09T17:56:09.208390",
     "exception": false,
     "start_time": "2023-07-09T17:56:09.187205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings = []\n",
    "    #GPU-WARM-UP\n",
    "    i=0\n",
    "    for data in test_loader:\n",
    "        if(i>1000):\n",
    "            break\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        _ = model(images)\n",
    "        i += 1\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            starter.record()\n",
    "            outputs = model(images)\n",
    "            ender.record()\n",
    "            \n",
    "            # WAIT FOR GPU SYNC\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            timings.append(curr_time)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the test images: '+str(100 * correct / total))\n",
    "    \n",
    "    tot = np.sum(timings)\n",
    "    mean_syn_per_batch = np.sum(timings) / len(timings)\n",
    "    std_syn_per_batch = np.std(timings)\n",
    "    print(\"Total inference time for test data: \"+str(tot))\n",
    "    print(\"Mean inference time per test batch: \"+str(mean_syn_per_batch))\n",
    "    print(\"Standard deviation of inference times per test batch: \"+str(std_syn_per_batch))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc899f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:09.222238Z",
     "iopub.status.busy": "2023-07-09T17:56:09.220691Z",
     "iopub.status.idle": "2023-07-09T17:56:14.156310Z",
     "shell.execute_reply": "2023-07-09T17:56:14.155339Z"
    },
    "papermill": {
     "duration": 4.944839,
     "end_time": "2023-07-09T17:56:14.158793",
     "exception": false,
     "start_time": "2023-07-09T17:56:09.213954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/chenyaofo/pytorch-cifar-models/releases/download/vgg/cifar10_vgg11_bn-eaeebf42.pt\" to /root/.cache/torch/hub/checkpoints/cifar10_vgg11_bn-eaeebf42.pt\n",
      "100%|██████████| 37.3M/37.3M [00:00<00:00, 136MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg11_bn\", pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "torch.save(model, \"./unpruned_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bf39b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:14.174737Z",
     "iopub.status.busy": "2023-07-09T17:56:14.173021Z",
     "iopub.status.idle": "2023-07-09T17:56:14.181644Z",
     "shell.execute_reply": "2023-07-09T17:56:14.180735Z"
    },
    "papermill": {
     "duration": 0.017301,
     "end_time": "2023-07-09T17:56:14.183592",
     "exception": false,
     "start_time": "2023-07-09T17:56:14.166291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU(inplace=True)\n",
      "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(model.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a23614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:14.199313Z",
     "iopub.status.busy": "2023-07-09T17:56:14.198950Z",
     "iopub.status.idle": "2023-07-09T17:56:20.407303Z",
     "shell.execute_reply": "2023-07-09T17:56:20.406318Z"
    },
    "papermill": {
     "duration": 6.219269,
     "end_time": "2023-07-09T17:56:20.409701",
     "exception": false,
     "start_time": "2023-07-09T17:56:14.190432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 102515277.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/cifar-10-python.tar.gz to ./datasets\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_transform = transforms.Compose([\n",
    "                                 transforms.Pad(4),\n",
    "                                 transforms.RandomCrop(32),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                             ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                             ])\n",
    "\n",
    "#######################\n",
    "\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10('./datasets', train=True, \n",
    "                                         download=True, transform=train_transform)\n",
    "test_set = torchvision.datasets.CIFAR10('./datasets', train=False, \n",
    "                                        download=True, transform=test_transform)\n",
    "# Number of subprocesses to use for data loading\n",
    "num_workers = 1\n",
    "# How many samples per batch to load\n",
    "batch_size = 1\n",
    "# Percentage of training set to use as validation\n",
    "valid_size = 0.5\n",
    "\n",
    "num_test = len(test_set)\n",
    "indices = list(range(num_test))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_test))\n",
    "test_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Define samplers for obtaining training and validation batches\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "# Prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n",
    "                                           num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(test_set, batch_size= 1, sampler=valid_sampler, \n",
    "                                           num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=2048, sampler=test_sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14581968",
   "metadata": {
    "papermill": {
     "duration": 0.007395,
     "end_time": "2023-07-09T17:56:20.425017",
     "exception": false,
     "start_time": "2023-07-09T17:56:20.417622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Testing the unpruned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c009f450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:20.441214Z",
     "iopub.status.busy": "2023-07-09T17:56:20.440900Z",
     "iopub.status.idle": "2023-07-09T17:56:29.748789Z",
     "shell.execute_reply": "2023-07-09T17:56:29.747636Z"
    },
    "papermill": {
     "duration": 9.318751,
     "end_time": "2023-07-09T17:56:29.751288",
     "exception": false,
     "start_time": "2023-07-09T17:56:20.432537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.34\n",
      "Total inference time for test data: 306.04524993896484\n",
      "Mean inference time per test batch: 102.01508331298828\n",
      "Standard deviation of inference times per test batch: 35.22032878017075\n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d05d42b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:29.769585Z",
     "iopub.status.busy": "2023-07-09T17:56:29.768699Z",
     "iopub.status.idle": "2023-07-09T17:56:29.816147Z",
     "shell.execute_reply": "2023-07-09T17:56:29.814023Z"
    },
    "papermill": {
     "duration": 0.059729,
     "end_time": "2023-07-09T17:56:29.819180",
     "exception": false,
     "start_time": "2023-07-09T17:56:29.759451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff42f87e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:29.838754Z",
     "iopub.status.busy": "2023-07-09T17:56:29.838419Z",
     "iopub.status.idle": "2023-07-09T17:56:29.846368Z",
     "shell.execute_reply": "2023-07-09T17:56:29.845472Z"
    },
    "papermill": {
     "duration": 0.021003,
     "end_time": "2023-07-09T17:56:29.848516",
     "exception": false,
     "start_time": "2023-07-09T17:56:29.827513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ten2mat(tensor):\n",
    "    r,c,ch,f = tensor.shape\n",
    "    new_dim = [c,r*ch*f]\n",
    "    return np.reshape(tensor, new_dim)\n",
    "\n",
    "def eff_rank(matrix):\n",
    "    frob = np.linalg.norm(matrix, 'fro')\n",
    "    svals = svdvals(matrix)\n",
    "    S = max(svals)\n",
    "    r = frob/S\n",
    "    return (r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69dabfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:29.866150Z",
     "iopub.status.busy": "2023-07-09T17:56:29.865858Z",
     "iopub.status.idle": "2023-07-09T17:56:29.910941Z",
     "shell.execute_reply": "2023-07-09T17:56:29.910090Z"
    },
    "papermill": {
     "duration": 0.056783,
     "end_time": "2023-07-09T17:56:29.913218",
     "exception": false,
     "start_time": "2023-07-09T17:56:29.856435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#utility functions\n",
    "\n",
    "\n",
    "#reshape weight/feature tensor into a matrix\n",
    "def ten2mat(tensor):\n",
    "    r,c,ch,f = tensor.shape\n",
    "    new_dim = [c,r*ch*f]\n",
    "    return np.reshape(tensor, new_dim)\n",
    "################################################################################################################\n",
    "#Compute stable rank\n",
    "def eff_rank(matrix):\n",
    "    frob = np.linalg.norm(matrix, 'fro')\n",
    "    svals = svdvals(matrix)\n",
    "    S = max(svals)\n",
    "    r = frob/S\n",
    "    return (r)\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "# a dict to store the activations\n",
    "activation = {}\n",
    "def getActivation(name):\n",
    "  # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "#Function to get conv+bn feature means\n",
    "def get_mean_features(nc, num_classes, num_samples, conv_dict, model, dataiter):\n",
    "    \n",
    "    activation = {}\n",
    "    def getActivation(name):\n",
    "      # the hook signature\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    mat_features_list_conv = [ [] ]*(nc*num_classes)\n",
    "    mat_features_list_bn = [ [] ]*(nc*num_classes)\n",
    "\n",
    "    count = np.zeros((nc*num_classes))\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        try:\n",
    "            image,label = next(dataiter)\n",
    "        except StopIteration:\n",
    "            dataiter = iter(train_loader)\n",
    "            image,label = next(dataiter)\n",
    "        \n",
    "        \n",
    "        l2 =label.detach().numpy()\n",
    "        l2 = l2[0]\n",
    "        #print(l2)\n",
    "        for j in range(nc):#range(len(conv_dict)):\n",
    "            activation = {}\n",
    "            z1 = conv_dict[j]\n",
    "            z2 = z1 + 1\n",
    "\n",
    "            #print(z1,z2)\n",
    "            h1 = model.features[z1].register_forward_hook(getActivation('conv2d'))\n",
    "            h2 = model.features[z2].register_forward_hook(getActivation('bn'))\n",
    "\n",
    "            output = model(image.to(device))\n",
    "\n",
    "            Xx1 = activation['conv2d']\n",
    "            Xx2 = activation['bn']\n",
    "\n",
    "            Xx1 = Xx1.to(device = 'cpu')\n",
    "            Xx2 = Xx2.to(device = 'cpu')\n",
    "\n",
    "            Xx01 = Xx1.detach().numpy()\n",
    "            Xx02 = Xx2.detach().numpy()\n",
    "\n",
    "            temp_mat1 = ten2mat(Xx01)\n",
    "            temp_mat2 = ten2mat(Xx02)\n",
    "\n",
    "            temp_idx = int(num_classes*j + l2)\n",
    "\n",
    "            count[temp_idx]+=1\n",
    "            #print(i, np.linalg.norm(temp_mat1),np.linalg.norm(temp_mat2), np.linalg.norm(temp_mat3), l2, j, z1, temp_idx)\n",
    "\n",
    "            l31 = len(mat_features_list_conv[temp_idx])\n",
    "            l32 = len(mat_features_list_bn[temp_idx])\n",
    "\n",
    "            if l31 == 0:\n",
    "                mat_features_list_conv[temp_idx] = temp_mat1\n",
    "\n",
    "            else:\n",
    "                mat_features_list_conv[temp_idx] = mat_features_list_conv[temp_idx] + temp_mat1\n",
    "\n",
    "            if l32 == 0:\n",
    "                mat_features_list_bn[temp_idx] = temp_mat2\n",
    "\n",
    "            else:\n",
    "                mat_features_list_bn[temp_idx] = mat_features_list_bn[temp_idx] + temp_mat2\n",
    "\n",
    "\n",
    "            h1.remove()\n",
    "            h2.remove()\n",
    "       \n",
    "    for i in range(nc*num_classes):\n",
    "        mat_features_list_conv[i] = mat_features_list_conv[i] / count[i]\n",
    "        mat_features_list_bn[i] = mat_features_list_bn[i] / count[i]\n",
    "    \n",
    "    return mat_features_list_conv, mat_features_list_bn, count\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "#function to get differences between features\n",
    "\n",
    "def get_features_diff(nc, num_classes, mat_features_list, conv_dict):\n",
    "    \n",
    "    mat_diff_list = []\n",
    "    \n",
    "    ll = int(0.5 * num_classes * (num_classes - 1))\n",
    "\n",
    "\n",
    "    count_list = [ [[]]*ll ]*nc\n",
    "\n",
    "    for k in range(nc):\n",
    "        idx1 = 0\n",
    "        idx2 = num_classes*k\n",
    "        temp_list = [[]]*ll\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            for j in range(i+1,num_classes):\n",
    "                #print(idx1)\n",
    "                ti = i+idx2\n",
    "                tj = j+idx2\n",
    "                temp1 = mat_features_list[ti] - mat_features_list[tj]\n",
    "                \n",
    "                temp_list[idx1] = temp1\n",
    "\n",
    "                count_list[k][idx1]= (i,j)\n",
    "                idx1+=1\n",
    "        mat_diff_list.append([temp_list])\n",
    "\n",
    "        \n",
    "    return mat_diff_list\n",
    "\n",
    "#################################################################################################################\n",
    "        \n",
    "def get_min_list(nc, num_classes, mat_diff_list):\n",
    "    min_list = []\n",
    "    ll = int(0.5 * num_classes * (num_classes - 1))\n",
    "\n",
    "    for k in range(nc):\n",
    "        layer_list = mat_diff_list[k][0]\n",
    "\n",
    "\n",
    "        tempmat = layer_list[0]\n",
    "        \n",
    "        Y_conv = np.shape(tempmat)\n",
    "        num_channels = Y_conv[0]\n",
    "        #print(ll)\n",
    "        temp_min_list= [[]]*num_channels\n",
    "\n",
    "\n",
    "        for i in range(ll):\n",
    "            temp1  = layer_list[i]\n",
    "\n",
    "\n",
    "\n",
    "            temp1n = np.linalg.norm(temp1, axis=1)\n",
    "\n",
    "\n",
    "            for j in range(num_channels):           \n",
    "                if bool(temp_min_list[j]):\n",
    "                    if temp_min_list[j] > temp1n[j]:\n",
    "                        temp_min_list[j] = temp1n[j]                    \n",
    "                else:\n",
    "                    temp_min_list[j] = temp1n[j]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        min_list.append(temp_min_list)\n",
    "    return min_list\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def test(model, data_loader, device):\n",
    "    acc = 0  # TODO compute top1\n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (idx, (x, t)) in enumerate(data_loader):\n",
    "            x = model.forward(x.to(device))\n",
    "            t = t.to(device)\n",
    "            _, indices = torch.max(x, 1)\n",
    "            correct_samples += torch.sum(indices == t)\n",
    "            total_samples += t.shape[0]\n",
    "\n",
    "    acc = float(correct_samples) / total_samples\n",
    "    return acc\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def get_crit_vals(nc, min_list, crit_frac):\n",
    "    minlist = []\n",
    "    min_means = []\n",
    "    min_crit = []\n",
    "    for i in range(nc):\n",
    "        temp_list = min_list[i] / np.max(min_list[i])\n",
    "        mean = np.mean(temp_list)\n",
    "\n",
    "\n",
    "        #print(i,mean_conv, mean_bn)\n",
    "\n",
    "        crit = crit_frac* mean\n",
    "        minlist.append(np.min(temp_list))\n",
    "        min_means.append(mean)\n",
    "        min_crit.append(crit)\n",
    "    \n",
    "    return min_list, min_means, min_crit\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def get_pruning_mask2(nc, min_crit, min_list,model, conv_dict):\n",
    "    \n",
    "    Mask = [[]]*nc\n",
    "    filters_pruned = [[]]*nc\n",
    "    filters_remain = [[]]*nc\n",
    "    \n",
    "    for k in range(nc):\n",
    "\n",
    "\n",
    "                #Conv output\n",
    "        Scores= min_list[k] \n",
    "        Scores = Scores / np.max(Scores)\n",
    "        len_sc = int(len(Scores))\n",
    "\n",
    "\n",
    "        Criterion = min_crit[k]\n",
    "        mask = np.ones(len_sc)\n",
    "        for i in range(len(Scores)):\n",
    "            temp = Scores[i]-Criterion\n",
    "            if temp <= 0:\n",
    "                mask[i]= 0\n",
    "\n",
    "                \n",
    "        num_rem = int(np.sum(mask))\n",
    "        num_prn = int(len(mask) - num_rem)  \n",
    "        \n",
    "        filters_pruned[k] = num_prn\n",
    "        filters_remain[k] = num_rem\n",
    "\n",
    "        \n",
    "        Test_ten = model.features[conv_dict[k]].weight\n",
    "        \n",
    "        Test_ten = Test_ten.to(device = 'cpu')\n",
    "\n",
    "        Test_ten = Test_ten.detach().numpy()\n",
    "\n",
    "        r,c,ch,f = np.shape(Test_ten)\n",
    "        Test_Mask = [[]]*r\n",
    "        One_ten = np.ones((c,ch,f))\n",
    "        for i in range(r):\n",
    "            Test_Mask[i] = One_ten*mask[i]\n",
    "        X = torch.tensor(Test_Mask).to(device)\n",
    "       \n",
    "        \n",
    "       \n",
    "        \n",
    "        Mask[k] = X\n",
    "    return Mask, filters_pruned, filters_remain\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "def prune_with_mask(nc, model, X, conv_dict):\n",
    "    \n",
    "    for k in range(nc):\n",
    "        mm = prune.custom_from_mask(model.features[conv_dict[k]], name='weight', mask=X)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#################################################################################################################\n",
    "def get_pruned_acc(nc, min_crit, min_list,model, conv_dict,test_loader,device):\n",
    "    \n",
    "    filters_pruned = [[]]*nc\n",
    "    filters_remain = [[]]*nc\n",
    "           \n",
    "    for k in range(nc):\n",
    "        #print(idx)\n",
    "        \n",
    "        \n",
    "        #Conv output\n",
    "        Scores = min_list[k] \n",
    "        Scores = Scores / np.max(Scores)\n",
    "        len_sc = int(len(Scores))\n",
    "\n",
    "\n",
    "        Criterion = min_crit[k]\n",
    "        mask = np.ones(len_sc)\n",
    "        for i in range(len(Scores)):\n",
    "            temp = Scores[i]-Criterion\n",
    "            if temp <= 0:\n",
    "                mask[i]= 0\n",
    "\n",
    "                \n",
    "        num_rem = int(np.sum(mask))\n",
    "        num_prn = int(len(mask) - num_rem)  \n",
    "        \n",
    "        filters_pruned[k] = num_prn\n",
    "        filters_remain[k] = num_rem\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        Test_ten = model.features[conv_dict[k]].weight\n",
    "        \n",
    "        Test_ten = Test_ten.to(device = 'cpu')\n",
    "\n",
    "        Test_ten = Test_ten.detach().numpy()\n",
    "\n",
    "        r,c,ch,f = np.shape(Test_ten)\n",
    "        Test_Mask = [[]]*r\n",
    "        One_ten = np.ones((c,ch,f))\n",
    "        for i in range(r):\n",
    "            Test_Mask[i] = One_ten*mask[i]\n",
    "        X = torch.tensor(Test_Mask).to(device)\n",
    "       \n",
    "        m1 = prune.custom_from_mask( model.features[conv_dict[k]], name='weight', mask=X)\n",
    "        \n",
    "        \n",
    "        #print(model_accs1[k, idx])\n",
    "        #print(k, crit_frac, num_pruned, pf, T1)\n",
    "    \n",
    "    #device = torch.device('cuda')\n",
    "    \n",
    "    T1 = test(model, test_loader,device)\n",
    "    T1 = 100*T1\n",
    "    \n",
    "    return model, T1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0422e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T17:56:29.931281Z",
     "iopub.status.busy": "2023-07-09T17:56:29.930985Z",
     "iopub.status.idle": "2023-07-09T20:56:42.772916Z",
     "shell.execute_reply": "2023-07-09T20:56:42.771781Z"
    },
    "papermill": {
     "duration": 10812.903565,
     "end_time": "2023-07-09T20:56:42.825023",
     "exception": false,
     "start_time": "2023-07-09T17:56:29.921458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "/tmp/ipykernel_24/4180631273.py:267: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  X = torch.tensor(Test_Mask).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0 91.9 10.065406976744185 89.93459302325581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.475 1 92.30000000000001 8.502906976744185 91.49709302325581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.475 2 92.10000000000001 10.86482558139535 89.13517441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.475 3 91.94 12.463662790697674 87.53633720930233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44999999999999996 4 91.88 11.954941860465116 88.04505813953489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42499999999999993 5 91.92 12.17296511627907 87.82703488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3999999999999999 6 91.97999999999999 12.063953488372094 87.93604651162791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3749999999999999 7 91.9 11.809593023255815 88.19040697674419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34999999999999987 8 92.0 11.555232558139535 88.44476744186046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32499999999999984 9 92.08 11.446220930232558 88.55377906976744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32499999999999984 10 91.92 11.882267441860465 88.11773255813954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2999999999999998 11 92.0 11.736918604651162 88.26308139534883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2749999999999998 12 91.96 11.773255813953488 88.22674418604652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2499999999999998 13 92.08 11.591569767441861 88.40843023255815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2499999999999998 14 91.97999999999999 11.700581395348838 88.29941860465117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2249999999999998 15 92.08 11.700581395348838 88.29941860465117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2249999999999998 16 92.06 11.773255813953488 88.22674418604652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2249999999999998 17 92.04 11.845930232558139 88.15406976744185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2249999999999998 18 92.02 11.991279069767442 88.00872093023256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2249999999999998 19 92.02 12.100290697674419 87.89970930232558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2249999999999998 20 91.97999999999999 12.24563953488372 87.75436046511628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19999999999999982 21 92.02 12.100290697674419 87.89970930232558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19999999999999982 22 92.0 12.209302325581394 87.79069767441861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17499999999999982 23 92.02 12.17296511627907 87.82703488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17499999999999982 24 92.02 12.209302325581394 87.79069767441861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17499999999999982 25 92.02 12.281976744186046 87.71802325581396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17499999999999982 26 91.97999999999999 12.318313953488373 87.68168604651163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14999999999999983 27 92.02 12.318313953488373 87.68168604651163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14999999999999983 28 92.02 12.318313953488373 87.68168604651163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14999999999999983 29 92.02 12.318313953488373 87.68168604651163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14999999999999983 30 91.96 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 31 92.02 12.318313953488373 87.68168604651163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 32 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 33 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 34 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 35 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 36 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 37 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 38 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 39 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 40 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 41 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499999999999983 42 92.0 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 43 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 44 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 45 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 46 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 47 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 48 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 49 92.02 12.354651162790697 87.6453488372093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 50 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 51 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 52 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 53 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 54 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 55 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 56 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 57 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 58 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 59 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 60 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 61 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 62 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 63 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 64 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 65 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 66 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 67 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999984 68 91.97999999999999 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 69 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 70 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 71 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 72 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 73 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 74 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 75 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 76 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 77 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 78 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 79 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 80 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 81 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 82 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 83 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 84 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 85 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 86 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 87 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 88 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 89 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 90 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 91 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 92 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 93 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 94 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 95 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 96 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 97 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 98 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 99 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 100 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 101 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 102 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 103 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 104 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 105 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 106 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 107 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 108 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 109 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 110 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 111 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 112 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 113 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 114 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 115 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 116 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 117 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 118 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 119 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 120 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 121 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 122 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 123 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 124 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 125 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 126 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 127 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 128 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 129 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 130 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 131 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 132 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 133 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 134 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 135 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 136 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 137 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 138 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 139 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 140 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 141 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 142 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 143 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 144 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 145 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 146 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 147 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 148 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 149 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 150 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 151 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 152 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 153 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 154 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 155 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 156 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 157 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 158 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 159 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 160 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 161 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 162 92.02 12.390988372093023 87.60901162790698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 163 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 164 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 165 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 166 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 167 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 168 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 169 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 170 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 171 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 172 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 173 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07499999999999984 174 92.0 12.463662790697674 87.53633720930233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 175 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 176 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 177 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 178 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 179 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 180 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 181 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 182 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 183 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 184 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999999984 185 91.94 12.463662790697674 87.53633720930233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 186 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 187 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 188 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 189 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 190 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 191 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 192 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 193 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 194 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 195 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 196 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 197 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 198 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 199 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 200 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 201 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 202 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 203 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 204 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 205 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 206 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 207 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 208 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 209 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 210 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 211 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 212 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 213 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 214 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 215 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 216 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 217 92.02 12.42732558139535 87.57267441860465\n",
      "Error while downloading pretrained model... retrying...\n",
      "Error while downloading pretrained model... retrying...\n",
      "Error while downloading pretrained model... retrying...\n",
      "Error while downloading pretrained model... retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 218 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 219 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 220 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 221 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 222 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 223 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 224 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 225 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 226 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 227 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 228 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 229 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 230 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 231 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 232 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 233 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 234 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 235 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 236 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 237 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 238 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 239 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 240 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 241 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 242 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 243 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 244 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 245 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 246 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 247 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 248 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 249 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 250 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 251 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 252 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 253 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 254 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 255 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 256 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 257 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 258 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 259 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 260 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 261 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 262 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 263 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 264 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 265 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 266 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 267 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 268 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 269 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 270 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 271 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 272 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 273 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 274 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 275 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 276 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 277 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 278 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 279 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 280 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 281 92.02 12.42732558139535 87.57267441860465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024999999999999842 282 91.97999999999999 12.463662790697674 87.53633720930233\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import HTTPError\n",
    "\n",
    "U = 60\n",
    "\n",
    "# accs = np.zeros(U)\n",
    "\n",
    "accs = []\n",
    "\n",
    "# num_pruned = np.zeros(U)\n",
    "# num_remain = np.zeros(U)\n",
    "\n",
    "num_pruned = []\n",
    "num_remain = []\n",
    "\n",
    "\n",
    "# conv_dict = [0,3,7,10,14,17,20,24,27,30, 34, 37, 40]\n",
    "conv_dict = [0,4,8,11,15,18,22,25]\n",
    "# conv_dict = [0,3,7,10,14,17,20,23,27,30,33,36,40,43,46,49]\n",
    "\n",
    "nc = int(len(conv_dict))\n",
    "num_classes = 10\n",
    "num_samples = 1024\n",
    "\n",
    "fracs = [.99,.9,.8, .7, .65,.6,.5,.4,.3,.2,.1,0]\n",
    "Crit_frac = .5\n",
    "\n",
    "threshold = 92.00\n",
    "frac_count = 0\n",
    "accuracy = 100000\n",
    "uu=0\n",
    "\n",
    "stop_at = 400\n",
    "freq = {}\n",
    "\n",
    "while Crit_frac > 0:\n",
    "    #print(uu)\n",
    "    mat_features_list_conv, mat_features_list_bn, count = get_mean_features(nc, num_classes, num_samples, conv_dict, model, dataiter) \n",
    "    diff_list_conv = get_features_diff(nc, num_classes, mat_features_list_conv, conv_dict)\n",
    "    diff_list_bn = get_features_diff(nc, num_classes, mat_features_list_bn, conv_dict)\n",
    "    min_list_conv = get_min_list(nc, num_classes, diff_list_conv)\n",
    "    min_list_bn = get_min_list(nc, num_classes, diff_list_bn)\n",
    "   \n",
    "    minlist_bn, minmeans_bn, min_crit_bn = get_crit_vals(nc, min_list_bn, Crit_frac)\n",
    "    \n",
    "    flag = True\n",
    "    test_model1 = \"\"\n",
    "    while(flag):\n",
    "        try:\n",
    "            test_model1 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg11_bn\", pretrained=True)\n",
    "            flag = False\n",
    "            #test_model1 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=True)\n",
    "            #test_model1 = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg11_bn\", pretrained=True)\n",
    "        except HTTPError:\n",
    "            print(\"Error while downloading pretrained model... retrying...\")\n",
    "            flag = True\n",
    "\n",
    "\n",
    "    test_model1 = test_model1.to(device)\n",
    "    \n",
    "    Mask_bn, filters_pruned_bn, filters_remain_bn = get_pruning_mask2(nc, min_crit_bn, min_list_bn,test_model1, conv_dict)\n",
    "    \n",
    "#     num_pruned[uu] = np.sum(filters_pruned_bn)\n",
    "#     num_remain[uu] = np.sum(filters_remain_bn)\n",
    "    fpbn = np.sum(filters_pruned_bn)\n",
    "    frbn = np.sum(filters_remain_bn)\n",
    "    np.append(num_pruned, fpbn)\n",
    "    np.append(num_remain, frbn)\n",
    "    \n",
    "    pf = 100 * (fpbn / (fpbn + frbn))\n",
    "    \n",
    "    for k in range(nc):\n",
    "        mm = prune.custom_from_mask( test_model1.features[conv_dict[k]], name='weight', mask=Mask_bn[k])\n",
    "        mm = prune.remove(mm, name='weight')\n",
    "    \n",
    "    T2 = test(test_model1, test_loader, device)\n",
    "    accuracy = T2 * 100\n",
    "    #accs[uu] = T2 *100\n",
    "    np.append(accs,accuracy)\n",
    "    \n",
    "    if accuracy > threshold:\n",
    "        for k in range(nc):\n",
    "            mmm = prune.custom_from_mask( model.features[conv_dict[k]], name='weight', mask=Mask_bn[k])\n",
    "            mmm = prune.remove(mmm, name='weight')\n",
    "            \n",
    "        \n",
    "        print(Crit_frac, uu, T2*100, pf, 100-pf)\n",
    "    else:\n",
    "        print(Crit_frac, uu, T2*100, pf, 100-pf)\n",
    "        frac_count +=1\n",
    "        Crit_frac = np.maximum(0, Crit_frac - 0.025)\n",
    "        \n",
    "    uu +=1\n",
    "    \n",
    "    if Crit_frac in freq:\n",
    "        freq[Crit_frac] += 1\n",
    "        if freq[Crit_frac] == stop_at:\n",
    "            break\n",
    "    else:\n",
    "        freq[Crit_frac] = 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecfbe95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T20:56:42.928038Z",
     "iopub.status.busy": "2023-07-09T20:56:42.927702Z",
     "iopub.status.idle": "2023-07-09T20:56:42.997840Z",
     "shell.execute_reply": "2023-07-09T20:56:42.996837Z"
    },
    "papermill": {
     "duration": 0.126214,
     "end_time": "2023-07-09T20:56:43.000149",
     "exception": false,
     "start_time": "2023-07-09T20:56:42.873935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"./pruned_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32aa1e",
   "metadata": {
    "papermill": {
     "duration": 0.050128,
     "end_time": "2023-07-09T20:56:43.101388",
     "exception": false,
     "start_time": "2023-07-09T20:56:43.051260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Testing the pruned model (with only zeroed out weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30d0ab96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T20:56:43.200119Z",
     "iopub.status.busy": "2023-07-09T20:56:43.199771Z",
     "iopub.status.idle": "2023-07-09T20:56:46.046698Z",
     "shell.execute_reply": "2023-07-09T20:56:46.045586Z"
    },
    "papermill": {
     "duration": 2.898773,
     "end_time": "2023-07-09T20:56:46.048884",
     "exception": false,
     "start_time": "2023-07-09T20:56:43.150111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 81.7\n",
      "Total inference time for test data: 343.84238052368164\n",
      "Mean inference time per test batch: 114.61412684122722\n",
      "Standard deviation of inference times per test batch: 41.75067729054353\n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d46e6f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T20:56:46.153450Z",
     "iopub.status.busy": "2023-07-09T20:56:46.153120Z",
     "iopub.status.idle": "2023-07-09T20:56:46.160425Z",
     "shell.execute_reply": "2023-07-09T20:56:46.159565Z"
    },
    "papermill": {
     "duration": 0.062855,
     "end_time": "2023-07-09T20:56:46.162434",
     "exception": false,
     "start_time": "2023-07-09T20:56:46.099579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a_file = open(\"test_vgg11_cifar10.txt\", \"a\")\n",
    "\n",
    "\n",
    "np.savetxt(a_file, [filters_pruned_bn], fmt = '%1.1i')\n",
    "\n",
    "np.savetxt(a_file, [filters_remain_bn], fmt = '%1.1i')\n",
    "\n",
    "np.savetxt(a_file, [accuracy], fmt = '%1.4f')\n",
    "\n",
    "\n",
    "a_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa504a",
   "metadata": {
    "papermill": {
     "duration": 0.048397,
     "end_time": "2023-07-09T20:56:46.259524",
     "exception": false,
     "start_time": "2023-07-09T20:56:46.211127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Performing architecture modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6096f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T20:56:46.358171Z",
     "iopub.status.busy": "2023-07-09T20:56:46.357199Z",
     "iopub.status.idle": "2023-07-09T20:56:59.412351Z",
     "shell.execute_reply": "2023-07-09T20:56:59.411141Z"
    },
    "papermill": {
     "duration": 13.107372,
     "end_time": "2023-07-09T20:56:59.415515",
     "exception": false,
     "start_time": "2023-07-09T20:56:46.308143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-pruning\r\n",
      "  Downloading torch_pruning-1.1.9-py3-none-any.whl (39 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torch-pruning) (2.0.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-pruning) (1.23.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torch-pruning) (3.12.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torch-pruning) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torch-pruning) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torch-pruning) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torch-pruning) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torch-pruning) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torch-pruning) (1.3.0)\r\n",
      "Installing collected packages: torch-pruning\r\n",
      "Successfully installed torch-pruning-1.1.9\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch-pruning\n",
    "import torch_pruning as tp\n",
    "    \n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d): #Iterating over all the conv2d layers of the model\n",
    "        channel_indices = [] #Stores indices of the channels to prune within this conv layer\n",
    "        t = module.weight.clone().detach()\n",
    "        t = t.reshape(t.shape[0], -1)\n",
    "        z = torch.all(t == 0, dim=1)\n",
    "        z = z.tolist()\n",
    "        \n",
    "        for i, flag in enumerate(z):\n",
    "            if(flag):\n",
    "                channel_indices.append(i)\n",
    "\n",
    "        if(channel_indices == []):\n",
    "            continue\n",
    "        \n",
    "        # 1. build dependency graph for vgg\n",
    "        DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,32,32).to(device))\n",
    "\n",
    "        # 2. Specify the to-be-pruned channels. Here we prune those channels indexed by idxs.\n",
    "        group = DG.get_pruning_group(module, tp.prune_conv_out_channels, idxs=channel_indices)\n",
    "        #print(group)\n",
    "\n",
    "        # 3. prune all grouped layers that are coupled with the conv layer (included).\n",
    "        if DG.check_pruning_group(group): # avoid full pruning, i.e., channels=0.\n",
    "            group.prune()\n",
    "    \n",
    "# 4. Save & Load\n",
    "model.zero_grad() # We don't want to store gradient information\n",
    "torch.save(model, './arch_pruned_model.pth') # without .state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765166d",
   "metadata": {
    "papermill": {
     "duration": 0.049936,
     "end_time": "2023-07-09T20:56:59.519391",
     "exception": false,
     "start_time": "2023-07-09T20:56:59.469455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Testing the pruned model (after architecture modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7125000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T20:56:59.624334Z",
     "iopub.status.busy": "2023-07-09T20:56:59.623204Z",
     "iopub.status.idle": "2023-07-09T20:57:03.336463Z",
     "shell.execute_reply": "2023-07-09T20:57:03.335168Z"
    },
    "papermill": {
     "duration": 3.768504,
     "end_time": "2023-07-09T20:57:03.338719",
     "exception": false,
     "start_time": "2023-07-09T20:56:59.570215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 81.58\n",
      "Total inference time for test data: 315.049373626709\n",
      "Mean inference time per test batch: 105.01645787556966\n",
      "Standard deviation of inference times per test batch: 34.3397031397702\n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10874.299854,
   "end_time": "2023-07-09T20:57:06.120519",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-09T17:55:51.820665",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
